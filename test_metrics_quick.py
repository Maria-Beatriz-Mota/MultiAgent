"""
Teste RÃ¡pido de MÃ©tricas - Multi-Agent IRIS System
====================================================
Script para executar testes rÃ¡pidos do sistema de mÃ©tricas.

Uso:
    python test_metrics_quick.py

Autor: Sistema Multi-Agente IRIS
Data: Dezembro 2025
"""

import sys
import os
import json
from datetime import datetime

# Adicionar diretÃ³rio pai ao path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    # Tenta importar com cuidado para evitar problemas de dependÃªncias
    import importlib.util
    
    # Importar RetrievalMetrics diretamente
    spec_retrieval = importlib.util.spec_from_file_location(
        "rag_metrics_retrieval",
        os.path.join(os.path.dirname(__file__), "Agent_C", "rag_metrics_retrieval.py")
    )
    rag_metrics_retrieval = importlib.util.module_from_spec(spec_retrieval)
    spec_retrieval.loader.exec_module(rag_metrics_retrieval)
    RetrievalMetrics = rag_metrics_retrieval.RetrievalMetrics
    
    # Importar RAGEvaluator
    spec_evaluator = importlib.util.spec_from_file_location(
        "rag_evaluator",
        os.path.join(os.path.dirname(__file__), "Agent_C", "rag_evaluator.py")
    )
    rag_evaluator = importlib.util.module_from_spec(spec_evaluator)
    spec_evaluator.loader.exec_module(rag_evaluator)
    RAGEvaluator = rag_evaluator.RAGEvaluator
    
except Exception as e:
    print(f"âš ï¸  Erro ao carregar mÃ³dulos: {e}")
    print("Continuando com modo limitado...")
    RetrievalMetrics = None
    RAGEvaluator = None


def test_retrieval_metrics():
    """Testa mÃ©tricas de retrieval com dados exemplo."""
    print("\n" + "="*70)
    print("ğŸ” TESTE 1: MÃ©tricas de Retrieval")
    print("="*70)
    
    if not RetrievalMetrics:
        print("âŒ RetrievalMetrics nÃ£o disponÃ­vel")
        return False
    
    metrics = RetrievalMetrics()
    
    # Dados de teste
    relevant_docs = {"doc1", "doc3", "doc5", "doc8"}
    retrieved_docs = ["doc3", "doc1", "doc7", "doc5", "doc9", "doc2", "doc8", "doc4"]
    
    results = metrics.evaluate_query(
        relevant_docs=relevant_docs,
        retrieved_docs=retrieved_docs,
        k_values=[1, 3, 5, 10]
    )
    
    metrics.print_results(results, "Query Ãšnica - Retrieval Metrics")
    
    return True


def test_dataset_evaluation():
    """Testa avaliaÃ§Ã£o de dataset completo."""
    print("\n" + "="*70)
    print("ğŸ“Š TESTE 2: AvaliaÃ§Ã£o de Dataset")
    print("="*70)
    
    if not RetrievalMetrics:
        print("âŒ RetrievalMetrics nÃ£o disponÃ­vel")
        return False
    
    metrics = RetrievalMetrics()
    
    # Dataset de teste com 3 queries
    dataset = [
        {
            "query": "DRC em gatos",
            "relevant_docs": {"doc1", "doc3", "doc5"},
            "retrieved_docs": ["doc3", "doc1", "doc7", "doc5"]
        },
        {
            "query": "IRIS staging",
            "relevant_docs": {"doc2", "doc4", "doc6"},
            "retrieved_docs": ["doc4", "doc2", "doc8", "doc6", "doc9"]
        },
        {
            "query": "Creatinina e SDMA",
            "relevant_docs": {"doc5", "doc7"},
            "retrieved_docs": ["doc7", "doc5", "doc1"]
        }
    ]
    
    results = metrics.evaluate_dataset(dataset, k_values=[1, 3, 5, 10])
    
    metrics.print_results(results, "Dataset Completo - MÃ©dia de MÃ©tricas")
    
    return True


def test_rag_evaluator():
    """Testa RAGEvaluator com dados reais do CSV."""
    print("\n" + "="*70)
    print("â­ TESTE 3: RAG Evaluator Completo")
    print("="*70)
    
    if not RAGEvaluator:
        print("âŒ RAGEvaluator nÃ£o disponÃ­vel")
        return False
    
    evaluator = RAGEvaluator()
    
    try:
        # Carregar dados do CSV
        print("\nğŸ“‚ Carregando dados de validaÃ§Ã£o...")
        evaluator.carregar_validacoes()
        
        # Gerar relatÃ³rio completo
        print("ğŸ“ˆ Gerando relatÃ³rio completo...")
        relatorio = evaluator.gerar_relatorio_completo()
        
        print("\nâœ… RelatÃ³rio gerado com sucesso!")
        print(f"ğŸ“Š Total de validaÃ§Ãµes: {len(evaluator.validacoes)}")
        
        return True
    
    except FileNotFoundError:
        print("âš ï¸  CSV de validaÃ§Ãµes nÃ£o encontrado")
        return False
    
    except Exception as e:
        print(f"âŒ Erro ao testar RAGEvaluator: {str(e)}")
        return False


def test_metrics_summary():
    """Imprime resumo dos sistemas de mÃ©tricas disponÃ­veis."""
    print("\n" + "="*70)
    print("ğŸ“‹ Sistemas de MÃ©tricas DisponÃ­veis")
    print("="*70)
    
    print("""
    1. RetrievalMetrics (rag_metrics_retrieval.py)
       â”œâ”€â”€ recall_at_k: ProporÃ§Ã£o de docs relevantes recuperados
       â”œâ”€â”€ precision_at_k: ProporÃ§Ã£o de top-k que sÃ£o relevantes
       â”œâ”€â”€ mrr: PosiÃ§Ã£o do primeiro doc relevante
       â””â”€â”€ ndcg_at_k: Considerando relevÃ¢ncia e posiÃ§Ã£o
    
    2. GenerationMetrics (rag_metrics_generation.py)
       â”œâ”€â”€ answer_accuracy: AcurÃ¡cia usando LLM-as-a-judge
       â”œâ”€â”€ faithfulness: Fidelidade aos documentos
       â””â”€â”€ groundedness: ProporÃ§Ã£o fundamentada
    
    3. RAGEvaluator (rag_evaluator.py)
       â”œâ”€â”€ AcurÃ¡cia Geral
       â”œâ”€â”€ PrecisÃ£o por EstÃ¡gio IRIS
       â”œâ”€â”€ ConcordÃ¢ncia entre Agentes
       â”œâ”€â”€ EficÃ¡cia do RAG
       â”œâ”€â”€ DistribuiÃ§Ã£o por Caso
       â””â”€â”€ DistribuiÃ§Ã£o de ConfianÃ§a
    """)
    
    return True


def main():
    """Executa todos os testes."""
    print("\n" + "="*70)
    print("ğŸš€ TESTE RÃPIDO - SISTEMA DE MÃ‰TRICAS")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    tests = [
        ("Retrieval Metrics", test_retrieval_metrics),
        ("Dataset Evaluation", test_dataset_evaluation),
        ("RAG Evaluator", test_rag_evaluator),
        ("Metrics Summary", test_metrics_summary)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            print(f"\nâ–¶ï¸  Executando: {test_name}...")
            success = test_func()
            results.append({
                "test": test_name,
                "status": "âœ… PASSOU" if success else "âŒ FALHOU"
            })
        except Exception as e:
            print(f"\nâŒ Erro em {test_name}: {str(e)}")
            results.append({
                "test": test_name,
                "status": f"âŒ ERRO: {str(e)}"
            })
    
    # Resumo final
    print("\n" + "="*70)
    print("ğŸ“Š RESUMO DOS TESTES")
    print("="*70)
    
    for result in results:
        print(f"{result['test']:.<40} {result['status']}")
    
    print("\n" + "="*70)
    print("âœ¨ Testes ConcluÃ­dos!")
    print("="*70)


if __name__ == "__main__":
    main()
