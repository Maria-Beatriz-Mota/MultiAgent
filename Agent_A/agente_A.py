"""
Agente A ‚Äì Interpreta√ß√£o Cl√≠nica e Orquestra√ß√£o (CORRIGIDO)
-----------------------------------------------------------
Fluxo: A ‚Üí B ‚Üí C ‚Üí A

Responsabilidades:
1. Receber formul√°rio + texto livre
2. Normalizar dados cl√≠nicos
3. Enviar dados ao Agente B
4. Enviar resultado do B ao Agente C
5. Consolidar resposta final ao veterin√°rio (CORRIGIDO)
"""

import os
from typing import Dict, Any, Optional

# =====================================================================
# CONFIGURA√á√ÉO LLM - M√öLTIPLAS OP√á√ïES COM FALLBACK
# =====================================================================

llm = None
LLM_DISPONIVEL = False
LLM_PROVIDER = None

# Tentar m√∫ltiplos provedores em ordem de prefer√™ncia
providers_to_try = []

# 1. OpenAI (melhor qualidade, requer API key)
if os.environ.get("OPENAI_API_KEY"):
    providers_to_try.append(("openai", "OpenAI GPT-3.5"))

# 2. Groq (r√°pido e gratuito, requer API key)
if os.environ.get("GROQ_API_KEY"):
    providers_to_try.append(("groq", "Groq gpt-oss-120b"))

# 3. HuggingFace (gratuito, menos confi√°vel)
if os.environ.get("HUGGINGFACEHUB_API_TOKEN"):
    providers_to_try.append(("huggingface", "HuggingFace"))

# Tentar cada provedor
for provider, name in providers_to_try:
    try:
        if provider == "openai":
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)
            LLM_PROVIDER = "OpenAI"
            
        elif provider == "groq":
            from langchain_groq import ChatGroq
            llm = ChatGroq(model="openai/gpt-oss-120b", temperature=0.3)
            LLM_PROVIDER = "Groq"
            
        elif provider == "huggingface":
            from langchain_huggingface import HuggingFaceEndpoint
            llm = HuggingFaceEndpoint(
                repo_id="google/flan-t5-large",
                temperature=0.2,
                max_new_tokens=512,
                huggingfacehub_api_token=os.environ.get("HUGGINGFACEHUB_API_TOKEN")
            )
            LLM_PROVIDER = "HuggingFace"
        
        # Testar se funciona
        LLM_DISPONIVEL = True
        print(f"[AGENTE A] LLM {LLM_PROVIDER} configurado")
        break
        
    except Exception as e:
        print(f"[AGENTE A] {name} n√£o dispon√≠vel: {e}")
        continue

if not LLM_DISPONIVEL:
    print("[AGENTE A] Nenhum LLM dispon√≠vel - usando modo texto direto")
    print("[AGENTE A] Configure: OPENAI_API_KEY, GROQ_API_KEY ou HUGGINGFACEHUB_API_TOKEN")

def gerar_explicacao_clinica(
    resultado_b: Dict[str, Any],
    resultado_c: Dict[str, Any],
    dados_clinicos: Dict[str, Any]
) -> str:
    """
    !!FUN√á√ÉO DESATIVADA - N√ÉO USAR!!
    
    MOTIVO: LLM pode distorcer informa√ß√µes m√©dicas cr√≠ticas ao "humanizar" texto.
    O Agente C √© o validador cient√≠fico oficial - sua resposta j√° est√° correta
    e validada por RAG + regras IRIS. N√£o deve ser alterada.
    
    DECIS√ÉO DE ARQUITETURA:
    - Agente C = Validador cient√≠fico (resposta autoritativa)
    - LLM = Pode alucinar/modificar dados m√©dicos (RISCO)
    - Solu√ß√£o = Usar resposta original de C sem modifica√ß√µes
    
    Args:
        resultado_b: Resultado da infer√™ncia ontol√≥gica
        resultado_c: Resultado da valida√ß√£o (RAG + regras)
        dados_clinicos: Dados cl√≠nicos do paciente
    
    Returns:
        Texto organizado e humanizado baseado na valida√ß√£o de C
    """
    
    # Pegar a mensagem de valida√ß√£o do Agente C
    mensagem_c = resultado_c.get("resposta_clinica", "")
    estagio_final = resultado_c.get("estagio_final")
    valida_b = resultado_c.get("valida_b")
    
    # Se n√£o tem LLM, retornar direto a mensagem do C
    if not LLM_DISPONIVEL or llm is None:
        print("[AGENTE A] !! LLM n√£o dispon√≠vel, usando texto direto do Agente C")
        return mensagem_c
    
    # Construir prompt para humanizar o texto
    prompt = f"""Voc√™ √© um especialista em comunica√ß√£o veterin√°ria.

Sua tarefa √© reescrever a avalia√ß√£o cl√≠nica a seguir em um tom claro, profissional e emp√°tico para um veterin√°rio.

AVALIA√á√ÉO ORIGINAL DO SISTEMA DE VALIDA√á√ÉO:
{mensagem_c}

INSTRU√á√ïES:
- Mantenha todas as informa√ß√µes m√©dicas precisas
- Fa√ßa o texto fluir naturalmente em PORTUGU√äS BRASILEIRO
- Use linguagem veterin√°ria profissional
- Seja conciso (3-4 senten√ßas)
- Mantenha a conclus√£o do est√°gio IRIS
- RESPONDA SEMPRE EM PORTUGU√äS

Avalia√ß√£o reescrita em portugu√™s:"""
    
    try:
        print(f"[AGENTE A]...Humanizando texto com LLM ({LLM_PROVIDER})...")
        
        # Diferentes m√©todos de invoca√ß√£o por provider
        if LLM_PROVIDER in ["OpenAI", "Groq"]:
            from langchain_core.messages import HumanMessage
            resposta = llm.invoke([HumanMessage(content=prompt)])
            texto = resposta.content if hasattr(resposta, 'content') else str(resposta)
        else:
            resposta = llm.invoke(prompt)
            texto = resposta if isinstance(resposta, str) else str(resposta)
        
        print("[AGENTE A] Texto humanizado com sucesso")
        return texto.strip()
        
    except Exception as e:
        print(f"[AGENTE A] !! Erro ao humanizar com LLM: {str(e)[:100]}")
        # Fallback: retornar texto do C sem modifica√ß√£o
        return mensagem_c


def _gerar_explicacao_basica(
    resultado_b: Dict[str, Any],
    resultado_c: Dict[str, Any],
    dados_clinicos: Dict[str, Any]
) -> str:
    """Fallback: retorna diretamente a mensagem do Agente C"""
    return resultado_c.get("resposta_clinica", "Valida√ß√£o n√£o dispon√≠vel")


# =====================================================================
# HELPERS
# =====================================================================

def _to_float(value: Any) -> Optional[float]:
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str):
        v = value.strip().replace(",", ".")
        try:
            return float(v)
        except:
            return None
    return None


def _erro(msg: str) -> Dict[str, Any]:
    return {
        "estagio_final": None,
        "mensagem": msg,
        "plano_terapeutico": [],
        "alertas": [],
        "confianca": "BAIXA"
    }


# =====================================================================
# PROCESSAMENTO DE ENTRADA
# =====================================================================

def processar_input_usuario(
    formulario: Optional[Dict[str, Any]] = None,
    texto_livre: Optional[str] = ""
) -> Dict[str, Any]:

    formulario = formulario or {}

    dados = {
        "nome": formulario.get("nome"),
        "sexo": formulario.get("sexo"),
        "raca": formulario.get("raca"),
        "creatinina": _to_float(formulario.get("creatinina")),
        "sdma": _to_float(formulario.get("sdma")),
        "idade": _to_float(formulario.get("idade")),
        "peso": _to_float(formulario.get("peso")),
        "pressao_arterial": _to_float(formulario.get("pressao_arterial") or formulario.get("pressao")),
        "upc": _to_float(formulario.get("upc")),
        "sintomas": formulario.get("sintomas", ""),
        "comorbidades": formulario.get("comorbidades", ""),
        "question": (texto_livre or "").strip()
    }

    return dados


# =====================================================================
# CONSOLIDA√á√ÉO FINAL (B + C) - CORRIGIDA
# =====================================================================

def consolidar_resultados(
    resultado_b: Dict[str, Any],
    resultado_c: Dict[str, Any],
    dados_clinicos: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    Consolida resultados de B e C respeitando os 4 casos do Agente C
    Gera explica√ß√£o cl√≠nica usando LLM
    
    Casos:
    1. B e RAG concordam ‚Üí confian√ßa ALTA
    2. B n√£o inferiu, RAG tem info ‚Üí confian√ßa MODERADA  
    3. Discrep√¢ncia entre B e RAG ‚Üí INCONSIST√äNCIA (pedir nova avalia√ß√£o)
    4. Sem dados suficientes ‚Üí BAIXA confian√ßa
    """
    
    dados_clinicos = dados_clinicos or {}
    
    # !PRIORIZAR campos do Agente C (ele j√° consolidou tudo)!
    estagio_final = resultado_c.get("estagio_final")
    caso = resultado_c.get("caso")
    inconsistencia = resultado_c.get("inconsistencia", False)
    
    alertas = []
    
    # =====================================================================
    # CASO 3: INCONSIST√äNCIA/DISCREP√ÇNCIA - ERRO CR√çTICO
    # =====================================================================
    if caso == 3 or inconsistencia:
        return {
            "estagio_final": None,
            "mensagem": resultado_c.get("resposta_clinica", "Discrep√¢ncia detectada nos biomarcadores."),
            "plano_terapeutico": [],
            "alertas": [
                "!!!! INCONSIST√äNCIA CR√çTICA: Valores de creatinina e SDMA apresentam discrep√¢ncia significativa.",
                "A√ß√£o requerida: Repetir exames laboratoriais antes de prosseguir com o tratamento."
            ],
            "confianca": "INV√ÅLIDA",
            "caso": caso
        }
    
    # =====================================================================
    # CASO 4: DADOS INSUFICIENTES
    # =====================================================================
    if caso == 4 or not estagio_final:
        return {
            "estagio_final": None,
            "mensagem": resultado_c.get(
                "resposta_clinica",
                "Informa√ß√µes insuficientes para determinar o est√°gio IRIS."
            ),
            "plano_terapeutico": [],
            "alertas": [
                "!!!!Dados cl√≠nicos insuficientes.",
                "Por favor, forne√ßa valores de creatinina e/ou SDMA."
            ],
            "confianca": "BAIXA",
            "caso": caso
        }
    
    # =====================================================================
    # CASOS 1 e 2: CLASSIFICA√á√ÉO V√ÅLIDA
    # =====================================================================
    
    # USAR DIRETAMENTE A RESPOSTA DO AGENTE C (J√Å VALIDADA)
    # O Agente C √© o validador cient√≠fico - sua resposta n√£o deve ser alterada
    # LLM pode introduzir erros ou "alucinar" informa√ß√µes m√©dicas
    mensagem = resultado_c.get("resposta_clinica", "")
    
    print("[AGENTE A] Usando resposta validada do Agente C (sem LLM)")
    print("[AGENTE A] Resposta cient√≠fica preservada para garantir precis√£o")
    
    # Plano terap√™utico
    plano = resultado_c.get("tratamento_recomendado", [])
    
    # Confian√ßa baseada na valida√ß√£o
    valida_b = resultado_c.get("valida_b")
    if valida_b is True:
        confianca = "ALTA"
        if caso == 1:
            alertas.append("Infer√™ncia ontol√≥gica validada pela literatura cient√≠fica.")
    elif valida_b is None:
        confianca = "MODERADA"
        if caso == 2:
            alertas.append("Classifica√ß√£o baseada na literatura (ontologia n√£o inferiu est√°gio).")
    else:
        confianca = "MODERADA"
    
    return {
        "estagio_final": estagio_final,
        "subestagio_ap": resultado_c.get("subestagio_ap"),  # NOVO: Propagar subet√°gios
        "subestagio_ht": resultado_c.get("subestagio_ht"),  # NOVO: Propagar subet√°gios
        "mensagem": mensagem,
        "plano_terapeutico": plano,
        "alertas": alertas,
        "confianca": confianca,
        "caso": caso,
        
        # Metadados √∫teis
        "valida_b": valida_b,
        "num_docs_rag": resultado_c.get("num_docs", 0),
    }


# =====================================================================
# FORMATA√á√ÉO FINAL PARA O VETERIN√ÅRIO - CORRIGIDA
# =====================================================================

def formatar_resposta_final(resultado: Dict[str, Any], dados_clinicos: Dict[str, Any] = None) -> str:
    """
    Formata a resposta final para apresenta√ß√£o ao veterin√°rio
    """
    
    resposta = []
    resposta.append("Avalia√ß√£o Cl√≠nica ‚Äì Doen√ßa Renal Cr√¥nica Felina")
    resposta.append("=" * 70)
    
    # Adicionar informa√ß√µes do paciente se dispon√≠veis
    if dados_clinicos:
        info_paciente = []
        if dados_clinicos.get("nome"):
            info_paciente.append(f"Paciente: {dados_clinicos['nome']}")
        if dados_clinicos.get("sexo"):
            sexo_desc = "Macho" if dados_clinicos['sexo'] == "M" else "F√™mea" if dados_clinicos['sexo'] == "F" else dados_clinicos['sexo']
            info_paciente.append(f"Sexo: {sexo_desc}")
        if dados_clinicos.get("raca"):
            info_paciente.append(f"Ra√ßa: {dados_clinicos['raca']}")
        if dados_clinicos.get("idade"):
            info_paciente.append(f"Idade: {dados_clinicos['idade']} anos")
        if dados_clinicos.get("peso"):
            info_paciente.append(f"Peso: {dados_clinicos['peso']} kg")
        
        if info_paciente:
            resposta.append("")
            resposta.append("Dados do Paciente:")
            resposta.append("-" * 70)
            resposta.append("  ‚Ä¢ " + " | ".join(info_paciente))
            resposta.append("")
    
    # üî• CASO 3: Inconsist√™ncia - destaque especial
    if resultado.get("confianca") == "INV√ÅLIDA":
        resposta.append("")
        resposta.append("!!!!" + "="*66)
        resposta.append("!!!!  ATEN√á√ÉO: VALORES LABORATORIAIS INCONSISTENTES")
        resposta.append("!!!! " + "="*66)
        resposta.append("")
        resposta.append(resultado.get("mensagem", ""))
        resposta.append("")
        
        if resultado.get("alertas"):
            resposta.append("A√ß√µes Recomendadas:")
            for a in resultado["alertas"]:
                resposta.append(f"   {a}")
        
        resposta.append("")
        resposta.append("=" * 70)
        return "\n".join(resposta)
    
    # Est√°gio IRIS
    estagio = resultado.get("estagio_final")
    subestagio_ap = resultado.get("subestagio_ap")
    subestagio_ht = resultado.get("subestagio_ht")
    
    if estagio:
        linha_estagio = f"\n Est√°gio IRIS sugerido: {estagio}"
        
        # Adicionar subet√°gios se dispon√≠veis
        subetagios_str = []
        if subestagio_ap:
            ap_desc = {"AP0": "n√£o protein√∫rico", "AP1": "borderline protein√∫rico", "AP2": "protein√∫rico"}.get(subestagio_ap, subestagio_ap)
            subetagios_str.append(f"{subestagio_ap} ({ap_desc})")
        if subestagio_ht:
            ht_desc = {"HT0": "risco m√≠nimo", "HT1": "risco baixo", "HT2": "risco moderado", "HT3": "risco grave"}.get(subestagio_ht, subestagio_ht)
            subetagios_str.append(f"{subestagio_ht} ({ht_desc})")
        
        if subetagios_str:
            linha_estagio += f" ‚Äî Subet√°gios: {', '.join(subetagios_str)}"
        
        resposta.append(linha_estagio)
    else:
        resposta.append(f"\n!!!! Est√°gio IRIS: N√ÉO DETERMINADO")
    
    # Fundamenta√ß√£o cl√≠nica
    resposta.append("")
    resposta.append("Fundamenta√ß√£o:")
    resposta.append("-" * 70)
    mensagem = resultado.get("mensagem", "Nenhuma informa√ß√£o dispon√≠vel")
    resposta.append(mensagem)
    
    # Alertas
    if resultado.get("alertas"):
        resposta.append("")
        resposta.append("! OBSERVA√á√ïES !:")
        resposta.append("-" * 70)
        for a in resultado["alertas"]:
            resposta.append(f"  ‚Ä¢ {a}")
    
    # Plano terap√™utico
    if resultado.get("plano_terapeutico"):
        resposta.append("")
        resposta.append(" Recomenda√ß√µes Terap√™uticas:")
        resposta.append("-" * 70)
        for idx, item in enumerate(resultado["plano_terapeutico"], 1):
            resposta.append(f"  {idx}. {item}")
    
    # Confian√ßa
    resposta.append("")
    resposta.append("-" * 70)
    confianca = resultado.get("confianca", "BAIXA")
    emoji = "üü¢" if confianca == "ALTA" else "üü°" if confianca == "MODERADA" else "üî¥"
    resposta.append(f"{emoji} Confian√ßa da an√°lise: {confianca}")
    
    # Debug info (caso)
    caso = resultado.get("caso")
    if caso:
        resposta.append(f"üìä Caso: {caso}")
    
    resposta.append("=" * 70)
    
    return "\n".join(resposta)